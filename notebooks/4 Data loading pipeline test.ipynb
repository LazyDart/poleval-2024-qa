{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"../scripts\")\n",
    "\n",
    "from data_processing import poquad, processing\n",
    "from t5.load_t5 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, valid_df = poquad.load_poquad_manually_downloaded(\"../data/poquad-manually-processed/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = poquad.dataset_into_str_input(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_input = poquad.dataset_into_str_input(valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer, model = load_plt5(\"../models/plt5-original-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24609    kontekst: Bogurodzica (pieśń)  Bogurodzica w p...\n",
       "19841    kontekst: Płód arlekin  Rybia łuska arlekinowa...\n",
       "1227     kontekst: Daniel Barenboim  W latach 1967–1987...\n",
       "33835    kontekst: Admirał Fłota Sowietskogo Sojuza Kuz...\n",
       "42819    kontekst: Błędy w koszykówce  Faul (ang. foul;...\n",
       "                               ...                        \n",
       "42843    kontekst: Pancerniki typu Tosa  Stępkę pancern...\n",
       "14143    kontekst: Kirił Petkow (skoczek narciarski)  W...\n",
       "9572     kontekst: Litwa na Zimowych Igrzyskach Olimpij...\n",
       "29981    kontekst: Dziewanna drobnokwiatowa  Dziewanna ...\n",
       "17324    kontekst: Róża (film 2011)  Tadeusz zostaje sp...\n",
       "Name: input_text, Length: 100, dtype: object"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input[\"input_text\"].sample(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "5\n",
      "10\n",
      "24\n",
      "69\n"
     ]
    }
   ],
   "source": [
    "sample = train_input[\"target_text\"].sample(100)\n",
    "\n",
    "max_size = 0\n",
    "for i in range(len(sample)):\n",
    "    size = tokenizer(sample.iloc[i], return_tensors=\"pt\").input_ids.shape[1]\n",
    "    if size > max_size:\n",
    "        max_size = size\n",
    "        print(max_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import List, Dict, Any\n",
    "from dataclasses import dataclass\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorWithPadding, Seq2SeqTrainer, Seq2SeqTrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class T2TDataCollator(DataCollatorWithPadding):\n",
    "    def collate_batch(self, batch: List) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Take a list of samples from a Dataset and collate them into a batch.\n",
    "        Returns:\n",
    "            A dictionary of tensors\n",
    "        \"\"\"\n",
    "        input_ids = torch.stack([example['input_ids'] for example in batch])\n",
    "        lm_labels = torch.stack([example['target_ids'] for example in batch])\n",
    "        lm_labels[lm_labels[:, :] == 0] = -100\n",
    "        attention_mask = torch.stack([example['attention_mask'] for example in batch])\n",
    "        decoder_attention_mask = torch.stack([example['target_attention_mask'] for example in batch])\n",
    "\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'target_ids': lm_labels,\n",
    "            'target_attention_mask': decoder_attention_mask\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class T2TDataCollator:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        # Assuming 'batch' is a list of examples, where each example is a dictionary\n",
    "        # that includes 'input_ids' and 'labels' (target sequence) among other possible keys.\n",
    "        input_ids = [example[\"input_ids\"] for example in batch]\n",
    "        labels = [example[\"labels\"] for example in batch]\n",
    "        \n",
    "        # Tokenizer's pad method can handle padding of both input_ids and decoder_input_ids\n",
    "        batch = self.tokenizer.pad(\n",
    "            {\"input_ids\": input_ids},\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Ensure labels are also included and properly padded\n",
    "        batch[\"labels\"] = self.tokenizer.pad(\n",
    "            {\"input_ids\": labels},\n",
    "            return_tensors=\"pt\"\n",
    "        )[\"input_ids\"]\n",
    "        \n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = processing.TextDataset(train_input, tokenizer, 1024, 128)\n",
    "valid_dataset = processing.TextDataset(valid_input, tokenizer, 1024, 128)\n",
    "\n",
    "# Create DataCollator\n",
    "data_collator = T2TDataCollator(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = list(train_dataset.input_text.loc[[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kontekst: Konfederacja polsko-czechosłowacka Projekty konfederacji zaczęły się załamywać 5 sierpnia 1942. Ponownie wróciła kwestia monachijska, co uaktywniło się wymianą listów Ripka – Stroński. Natomiast 17 sierpnia 1942 doszło do spotkania E. Beneša i J. Masaryka z jednej a Wł. Sikorskiego i E. Raczyńskiego z drugiej strony. Polscy dyplomaci zaproponowali podpisanie układu konfederacyjnego. W następnym miesiącu, tj. 24 września, strona polska przesłała na ręce J. Masaryka projekt deklaracji o przyszłej konfederacji obu państw. Strona czechosłowacka projekt przyjęła, lecz już w listopadzie 1942 E. Beneš podważył ideę konfederacji. W zamian zaproponowano zawarcie układu sojuszniczego z Polską na 20 lat (formalnie nastąpiło to 20 listopada 1942). pytanie: Co było powodem powrócenia konceptu porozumieniu monachijskiego?']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(tokenizer.batch_encode_plus(\n",
    "            input_text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=1024,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            truncation=True\n",
    "        )[\"input_ids\"], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:hiat8wij) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9bbc7cd0aee421e87d7b243694f1ff6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.003 MB of 0.003 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁▃▆█</td></tr><tr><td>train/global_step</td><td>▁▃▆█</td></tr><tr><td>train/learning_rate</td><td>▁▁▁</td></tr><tr><td>train/loss</td><td>▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>total_flos</td><td>1.976354068443955e+16</td></tr><tr><td>train/epoch</td><td>0.99974</td></tr><tr><td>train/global_step</td><td>1924</td></tr><tr><td>train/grad_norm</td><td>nan</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.0</td></tr><tr><td>train_loss</td><td>0.0</td></tr><tr><td>train_runtime</td><td>2352.903</td></tr><tr><td>train_samples_per_second</td><td>19.63</td></tr><tr><td>train_steps_per_second</td><td>0.818</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">happy-durian-8</strong> at: <a href='https://wandb.ai/m-tkacz-uw/PLT5%20Small%20Finetuning%20Poquad/runs/hiat8wij' target=\"_blank\">https://wandb.ai/m-tkacz-uw/PLT5%20Small%20Finetuning%20Poquad/runs/hiat8wij</a><br/> View project at: <a href='https://wandb.ai/m-tkacz-uw/PLT5%20Small%20Finetuning%20Poquad' target=\"_blank\">https://wandb.ai/m-tkacz-uw/PLT5%20Small%20Finetuning%20Poquad</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240701_151145-hiat8wij/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:hiat8wij). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58954c8b15d84e77b07299d764b16e04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112396999993102, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/lazydart/Python Codes/PolEval2024QA/scripts/wandb/run-20240701_164346-awcvoq25</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/m-tkacz-uw/PLT5%20Small%20Finetuning%20Poquad/runs/awcvoq25' target=\"_blank\">major-bee-9</a></strong> to <a href='https://wandb.ai/m-tkacz-uw/PLT5%20Small%20Finetuning%20Poquad' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/m-tkacz-uw/PLT5%20Small%20Finetuning%20Poquad' target=\"_blank\">https://wandb.ai/m-tkacz-uw/PLT5%20Small%20Finetuning%20Poquad</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/m-tkacz-uw/PLT5%20Small%20Finetuning%20Poquad/runs/awcvoq25' target=\"_blank\">https://wandb.ai/m-tkacz-uw/PLT5%20Small%20Finetuning%20Poquad/runs/awcvoq25</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/m-tkacz-uw/PLT5%20Small%20Finetuning%20Poquad/runs/awcvoq25?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f36e112f140>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"PLT5 Small Finetuning Poquad\",\n",
    "    # track hyperparameters and run metadata\n",
    "    # config={\n",
    "    # \"architecture\": \"PLT5 Small\",\n",
    "    # \"dataset\": \"Poquad\",\n",
    "    # }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "\n",
    "class PrintMemoryUsageCallback(TrainerCallback):\n",
    "    \"\"\" Callback that prints memory allocation during training \"\"\"\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        print(f\"Step {state.global_step}: {torch.cuda.memory_allocated() / 1024 ** 2:.2f} MB allocated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c02e9de6ee94ff9a909559b745a0295",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: 380.25 MB allocated\n",
      "Step 2: 380.25 MB allocated\n",
      "Step 3: 380.25 MB allocated\n",
      "Step 4: 380.25 MB allocated\n",
      "Step 5: 380.25 MB allocated\n",
      "Step 6: 380.25 MB allocated\n",
      "Step 7: 380.25 MB allocated\n",
      "Step 8: 380.25 MB allocated\n",
      "Step 9: 380.25 MB allocated\n",
      "Step 10: 380.25 MB allocated\n",
      "Step 11: 380.25 MB allocated\n",
      "Step 12: 380.25 MB allocated\n",
      "Step 13: 380.25 MB allocated\n",
      "Step 14: 380.25 MB allocated\n",
      "Step 15: 380.25 MB allocated\n",
      "Step 16: 380.25 MB allocated\n",
      "Step 17: 380.25 MB allocated\n",
      "Step 18: 380.25 MB allocated\n",
      "Step 19: 380.25 MB allocated\n",
      "Step 20: 380.25 MB allocated\n",
      "Step 21: 380.25 MB allocated\n",
      "Step 22: 380.25 MB allocated\n",
      "Step 23: 380.25 MB allocated\n",
      "Step 24: 380.25 MB allocated\n",
      "Step 25: 380.25 MB allocated\n",
      "Step 26: 380.25 MB allocated\n",
      "Step 27: 380.25 MB allocated\n",
      "Step 28: 380.25 MB allocated\n",
      "Step 29: 380.25 MB allocated\n",
      "Step 30: 380.25 MB allocated\n",
      "Step 31: 380.25 MB allocated\n",
      "Step 32: 380.25 MB allocated\n",
      "Step 33: 380.25 MB allocated\n",
      "Step 34: 380.25 MB allocated\n",
      "Step 35: 380.25 MB allocated\n",
      "Step 36: 380.25 MB allocated\n",
      "Step 37: 380.25 MB allocated\n",
      "Step 38: 380.25 MB allocated\n",
      "Step 39: 380.25 MB allocated\n",
      "Step 40: 380.25 MB allocated\n",
      "Step 41: 380.25 MB allocated\n",
      "{'train_runtime': 46.081, 'train_samples_per_second': 21.701, 'train_steps_per_second': 0.89, 'train_loss': 0.0, 'epoch': 0.98}\n",
      "Training complete and model saved to ./trained_model\n"
     ]
    }
   ],
   "source": [
    "train_dataset = processing.TextDataset(train_input.iloc[:1000], tokenizer, 1024, 128)\n",
    "valid_dataset = processing.TextDataset(valid_input.iloc[:1000], tokenizer, 1024, 128)\n",
    "\n",
    "# Create DataCollator\n",
    "data_collator = T2TDataCollator(tokenizer)\n",
    "\n",
    "# Initialize model\n",
    "\n",
    "# Define TrainingArguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    learning_rate=3e-4,\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=6,\n",
    "    per_device_eval_batch_size=6,\n",
    "    save_total_limit=1,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,\n",
    "    gradient_accumulation_steps=4,\n",
    "    logging_dir='./logs',\n",
    "    overwrite_output_dir=True,\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=valid_dataset,\n",
    "        data_collator=data_collator,\n",
    "        callbacks=[PrintMemoryUsageCallback()],\n",
    "        )\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "trainer.save_model('../models/plt5-small-1epoch')\n",
    "\n",
    "print(\"Training complete and model saved to ./trained_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mltorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
